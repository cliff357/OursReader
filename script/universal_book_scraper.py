import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, urlparse
import os
from http.client import RemoteDisconnected  # æ­£ç¢ºçš„å°å…¥

class UniversalBookScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.delay = 60  # çˆ¬å–é–“éš”ï¼Œé¿å…è¢«å°
        
        # æ–°å¢é‡è©¦é…ç½®
        self.max_retries = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
        self.retry_delay = 10  # é‡è©¦å»¶é²ï¼ˆç§’ï¼‰
        
        # æ–°å¢çµ±è¨ˆè®Šé‡
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_chapters': 0,
            'total_pages': 0,
            'total_characters': 0,
            'total_words': 0,
            'failed_chapters': 0,
            'visited_urls': [],
            'successful_urls': [],
            'failed_urls': []
        }
        
        # æ–°å¢çºŒå‚³ç›¸é—œè®Šé‡
        self.existing_book_data = None
        self.existing_chapters = []
        self.existing_urls = set()
        self.continue_mode = False

        # æ–°å¢è‡ªå‹•æ¢å¾©é…ç½®
        self.auto_recovery = True  # æ˜¯å¦å•Ÿç”¨è‡ªå‹•æ¢å¾©
        self.recovery_delay = 60  # æ¢å¾©ç­‰å¾…æ™‚é–“ï¼ˆç§’ï¼‰
        self.max_recoveries = 5  # æœ€å¤§æ¢å¾©æ¬¡æ•¸
        self.recovery_count = 0  # ç•¶å‰æ¢å¾©æ¬¡æ•¸
    
    def scrape_from_url(self, start_url, max_chapters=999):
        """å¾æŒ‡å®šURLé–‹å§‹çˆ¬å–æ›¸ç±ï¼ˆæ”¯æ´çºŒå‚³å’Œè‡ªå‹•æ¢å¾©ï¼‰"""
        # åˆå§‹åŒ–çµ±è¨ˆ
        self.stats['start_time'] = time.time()
        self.stats['visited_urls'] = []
        self.stats['successful_urls'] = []
        self.stats['failed_urls'] = []
        
        print(f"ğŸš€ é–‹å§‹å¾URLçˆ¬å–ï¼š{start_url}")
        
        # ğŸ” æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰çš„æ›¸ç±æ–‡ä»¶
        existing_file = self.check_existing_book_file(start_url)
        chapters = []
        
        if (existing_file):
            print(f"ğŸ“– ç™¼ç¾ç¾æœ‰æ›¸ç±æ–‡ä»¶ï¼š{existing_file}")
            chapters = self.load_existing_chapters(existing_file)
            if chapters:
                print(f"âœ… è¼‰å…¥äº† {len(chapters)} å€‹å·²å­˜åœ¨çš„ç« ç¯€")
                self.continue_mode = True
                
                # æ‰¾åˆ°æ‡‰è©²ç¹¼çºŒçš„URL
                continue_url = self.find_continue_url(start_url, chapters)
                if continue_url:
                    start_url = continue_url
                    print(f"ğŸ”— çºŒå‚³æ¨¡å¼ï¼šå¾ç¬¬ {len(chapters) + 1} ç« é–‹å§‹ï¼š{continue_url}")
                else:
                    print("âœ… æ‰€æœ‰ç« ç¯€å·²å®Œæˆï¼Œç„¡éœ€ç¹¼çºŒçˆ¬å–")
                    return self.existing_book_data
        
        print(f"ğŸ“š æœ€å¤šçˆ¬å– {max_chapters} ç« ")
        print(f"â° é–‹å§‹æ™‚é–“ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}")
        if self.continue_mode:
            print(f"ğŸ”„ çºŒå‚³æ¨¡å¼ï¼šå·²æœ‰ {len(chapters)} ç« ï¼Œç¹¼çºŒçˆ¬å–æ–°ç« ç¯€")
        print("-" * 60)
        
        current_url = start_url
        chapter_count = len(chapters)  # å¾å·²æœ‰ç« ç¯€æ•¸é–‹å§‹è¨ˆç®—
        visited_urls = self.existing_urls.copy()  # åŒ…å«å·²å­˜åœ¨çš„URLs
        
        while current_url and chapter_count < max_chapters:
            # é˜²æ­¢é‡è¤‡çˆ¬å–
            if current_url in visited_urls:
                print(f"âš ï¸ æª¢æ¸¬åˆ°é‡è¤‡URLï¼Œåœæ­¢çˆ¬å–ï¼š{current_url}")
                break
                
            visited_urls.add(current_url)
            self.stats['visited_urls'].append(current_url)
            
            # ä½¿ç”¨é‡è©¦æ©Ÿåˆ¶çˆ¬å–ç« ç¯€
            success = self.scrape_chapter_with_retry(
                current_url, chapter_count + 1, chapters
            )
            
            if success:
                # é‡ç½®æ¢å¾©è¨ˆæ•¸å™¨ï¼ˆæˆåŠŸå¾Œé‡ç½®ï¼‰
                self.recovery_count = 0
                
                # å°‹æ‰¾ä¸‹ä¸€ç« é€£çµ - åŠ å…¥è‡ªå‹•æ¢å¾©æ©Ÿåˆ¶
                next_url_result = self.find_next_page_with_recovery(current_url)
                
                if next_url_result is None:
                    # è‡ªå‹•æ¢å¾©å¤±æ•—ï¼Œåœæ­¢çˆ¬å–
                    print("ğŸ’¾ è‡ªå‹•æ¢å¾©å¤±æ•—ï¼Œæ­£åœ¨ä¿å­˜å·²ç²å–çš„å…§å®¹...")
                    break
                elif next_url_result == "completed":
                    # æ­£å¸¸å®Œæˆï¼Œæ²’æœ‰ä¸‹ä¸€ç« 
                    print("ğŸ“„ æ²’æœ‰æ‰¾åˆ°ä¸‹ä¸€ç« é€£çµï¼Œçˆ¬å–å®Œæˆ")
                    break
                else:
                    # æˆåŠŸæ‰¾åˆ°ä¸‹ä¸€ç« 
                    current_url = next_url_result
                    chapter_count += 1
                    print(f"ğŸ”— æ‰¾åˆ°ä¸‹ä¸€ç« ï¼š{next_url_result}")
                    time.sleep(self.delay)
            else:
                # ç« ç¯€çˆ¬å–å¤±æ•—ï¼Œä½†å…ˆä¿å­˜å·²ç²å–çš„å…§å®¹
                print("ğŸ’¾ çˆ¬å–ä¸­æ–·ï¼Œæ­£åœ¨ä¿å­˜å·²ç²å–çš„å…§å®¹...")
                break
        
        # å®Œæˆçµ±è¨ˆ
        self.stats['end_time'] = time.time()
        self.stats['total_chapters'] = len(chapters)
        
        # é¡¯ç¤ºçˆ¬å–ç¸½çµ
        self.print_scraping_summary(chapters)
        
        if self.continue_mode:
            print(f"ğŸ‰ çºŒå‚³å®Œæˆï¼ç¸½å…± {len(chapters)} ç« ï¼ˆæ–°å¢ {len(chapters) - len(self.existing_chapters)} ç« ï¼‰")
        else:
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±çˆ¬å– {len(chapters)} ç« ")
        
        # å¦‚æœçˆ¬å–åˆ°å…§å®¹ï¼Œå˜—è©¦æå–æ›¸åå’Œä½œè€…
        if chapters:
            if self.continue_mode and self.existing_book_data:
                # çºŒå‚³æ¨¡å¼ï¼šæ›´æ–°ç¾æœ‰æ›¸ç±æ•¸æ“š
                book_title = self.existing_book_data['title']
                author = self.existing_book_data['author']
            else:
                # æ–°æ›¸æ¨¡å¼ï¼šæå–æ›¸åå’Œä½œè€…
                book_title, author = self.extract_book_info(start_url, chapters[0])
            
            ebook_data = self.convert_to_ebook(book_title, author, chapters)
            self.stats['total_pages'] = len(ebook_data['pages'])
            
            # è‡ªå‹•ä¿å­˜ï¼ˆçºŒå‚³æ¨¡å¼ä¸‹æœƒè¦†è“‹åŸæ–‡ä»¶ï¼‰
            is_complete = (chapter_count >= max_chapters or current_url is None)
            self.auto_save_book(ebook_data, is_complete, is_continue=self.continue_mode)
            
            return ebook_data
        else:
            print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•ç« ç¯€")
            return None

    def find_next_page_with_recovery(self, current_url):
        """å°‹æ‰¾ä¸‹ä¸€ç« é€£çµï¼Œæ”¯æ´è‡ªå‹•æ¢å¾©æ©Ÿåˆ¶"""
        for attempt in range(self.max_retries + 1):
            try:
                print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ä¸‹ä¸€ç« é€£çµï¼š{current_url}")
                if attempt > 0:
                    print(f"   ğŸ”„ æŸ¥æ‰¾é‡è©¦ç¬¬ {attempt} æ¬¡...")
                
                response = self.session.get(current_url, timeout=10)
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                next_url = self.find_next_page_url(soup, current_url)
                
                if next_url:
                    return next_url
                else:
                    return "completed"  # æ­£å¸¸å®Œæˆï¼Œæ²’æœ‰ä¸‹ä¸€ç« 
                    
            except (requests.exceptions.ConnectionError, 
                    RemoteDisconnected,  # ä¿®æ­£ï¼šç§»é™¤ requests.exceptions.
                    ConnectionError) as e:
                print(f"âŒ é€£æ¥éŒ¯èª¤ (æŸ¥æ‰¾ä¸‹ä¸€ç«  {attempt + 1}/{self.max_retries + 1}): {e}")
                
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    # é”åˆ°é‡è©¦ä¸Šé™ï¼Œå•Ÿå‹•è‡ªå‹•æ¢å¾©
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        return self.trigger_auto_recovery(current_url, "find_next_page")
                    else:
                        print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸å’Œæ¢å¾©æ¬¡æ•¸ï¼Œåœæ­¢çˆ¬å–")
                        return None
                        
            except requests.exceptions.Timeout as e:
                print(f"âŒ è«‹æ±‚è¶…æ™‚ (æŸ¥æ‰¾ä¸‹ä¸€ç«  {attempt + 1}/{self.max_retries + 1}): {e}")
                
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        return self.trigger_auto_recovery(current_url, "find_next_page")
                    else:
                        print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸å’Œæ¢å¾©æ¬¡æ•¸ï¼Œåœæ­¢çˆ¬å–")
                        return None
                        
            except Exception as e:
                print(f"âŒ å…¶ä»–éŒ¯èª¤ (æŸ¥æ‰¾ä¸‹ä¸€ç«  {attempt + 1}/{self.max_retries + 1}): {e}")
                
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        return self.trigger_auto_recovery(current_url, "find_next_page")
                    else:
                        print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸å’Œæ¢å¾©æ¬¡æ•¸ï¼Œåœæ­¢çˆ¬å–")
                        return None
        
        return None

    def trigger_auto_recovery(self, failed_url, operation_type):
        """è§¸ç™¼è‡ªå‹•æ¢å¾©æ©Ÿåˆ¶"""
        self.recovery_count += 1
        
        print(f"\nğŸš¨ === è‡ªå‹•æ¢å¾©æ©Ÿåˆ¶å•Ÿå‹• (ç¬¬ {self.recovery_count} æ¬¡) ===")
        print(f"âŒ æ“ä½œå¤±æ•—ï¼š{operation_type}")
        print(f"ğŸ”— å¤±æ•—URLï¼š{failed_url}")
        print(f"â° ç­‰å¾…æ™‚é–“ï¼š{self.recovery_delay} ç§’")
        print(f"ğŸ“Š å‰©é¤˜æ¢å¾©æ¬¡æ•¸ï¼š{self.max_recoveries - self.recovery_count}")
        print("=" * 60)
        
        # é¡¯ç¤ºå€’æ•¸è¨ˆæ™‚
        for remaining in range(self.recovery_delay, 0, -1):
            if remaining % 10 == 0 or remaining <= 10:
                print(f"â³ è‡ªå‹•æ¢å¾©å€’æ•¸ï¼š{remaining} ç§’...")
            time.sleep(1)
        
        print("ğŸ”„ è‡ªå‹•æ¢å¾©é–‹å§‹ï¼Œé‡æ–°å˜—è©¦æ“ä½œ...")
        
        # é‡æ–°å»ºç«‹é€£æ¥
        self.reset_session()
        
        # æ ¹æ“šæ“ä½œé¡å‹é‡æ–°å˜—è©¦
        if operation_type == "find_next_page":
            return self.retry_find_next_page(failed_url)
        elif operation_type == "scrape_chapter":
            return self.retry_scrape_chapter(failed_url)
        
        return None

    def reset_session(self):
        """é‡æ–°å»ºç«‹æœƒè©±é€£æ¥"""
        print("ğŸ”„ é‡æ–°å»ºç«‹ç¶²çµ¡é€£æ¥...")
        
        # é—œé–‰èˆŠçš„æœƒè©±
        self.session.close()
        
        # å‰µå»ºæ–°çš„æœƒè©±
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        print("âœ… ç¶²çµ¡é€£æ¥é‡æ–°å»ºç«‹")

    def retry_find_next_page(self, url):
        """æ¢å¾©å¾Œé‡æ–°å˜—è©¦æŸ¥æ‰¾ä¸‹ä¸€ç« """
        try:
            print(f"ğŸ” æ¢å¾©ï¼šé‡æ–°æŸ¥æ‰¾ä¸‹ä¸€ç« é€£çµï¼š{url}")
            
            response = self.session.get(url, timeout=15)
            response.encoding = response.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            next_url = self.find_next_page_url(soup, url)
            
            if next_url:
                print(f"âœ… æ¢å¾©æˆåŠŸï¼šæ‰¾åˆ°ä¸‹ä¸€ç« ï¼š{next_url}")
                return next_url
            else:
                print("âœ… æ¢å¾©æˆåŠŸï¼šç¢ºèªæ²’æœ‰æ›´å¤šç« ç¯€")
                return "completed"
                
        except Exception as e:
            print(f"âŒ æ¢å¾©å¤±æ•—ï¼š{e}")
            
            # å¦‚æœé‚„æœ‰æ¢å¾©æ©Ÿæœƒï¼Œå†æ¬¡å˜—è©¦
            if self.recovery_count < self.max_recoveries:
                print("ğŸ”„ å°‡å†æ¬¡å˜—è©¦è‡ªå‹•æ¢å¾©...")
                return self.trigger_auto_recovery(url, "find_next_page")
            else:
                print("âŒ å·²é”åˆ°æœ€å¤§æ¢å¾©æ¬¡æ•¸ï¼Œæ”¾æ£„æ¢å¾©")
                return None

    def retry_scrape_chapter(self, url):
        """æ¢å¾©å¾Œé‡æ–°å˜—è©¦çˆ¬å–ç« ç¯€"""
        try:
            print(f"ğŸ“– æ¢å¾©ï¼šé‡æ–°çˆ¬å–ç« ç¯€ï¼š{url}")
            
            response = self.session.get(url, timeout=15)
            response.encoding = response.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # é€™è£¡å¯ä»¥é‡æ–°çˆ¬å–ç« ç¯€ï¼Œä½†ç”±æ–¼å‡½æ•¸çµæ§‹é™åˆ¶ï¼Œ
            # æˆ‘å€‘è¿”å› True è¡¨ç¤ºå¯ä»¥ç¹¼çºŒï¼Œè®“ä¸»å¾ªç’°é‡æ–°è™•ç†
            print("âœ… æ¢å¾©æˆåŠŸï¼šé‡æ–°å»ºç«‹é€£æ¥ï¼Œå¯ä»¥ç¹¼çºŒçˆ¬å–")
            return True
            
        except Exception as e:
            print(f"âŒ ç« ç¯€æ¢å¾©å¤±æ•—ï¼š{e}")
            return False

    def scrape_chapter_with_retry(self, url, chapter_num, chapters):
        """ä½¿ç”¨é‡è©¦æ©Ÿåˆ¶çˆ¬å–å–®å€‹ç« ç¯€ï¼ˆåŠ å¼·éŒ¯èª¤è™•ç†ï¼‰"""
        for attempt in range(self.max_retries + 1):
            try:
                print(f"ğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {chapter_num} ç« ï¼š{url}")
                if attempt > 0:
                    print(f"   ğŸ”„ é‡è©¦ç¬¬ {attempt} æ¬¡...")
                
                # ç²å–é é¢
                response = self.session.get(url, timeout=15)
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ™ºèƒ½æå–ç« ç¯€æ¨™é¡Œå’Œå…§å®¹
                chapter_title, content = self.extract_chapter_content(soup, chapter_num)
                
                if content.strip():
                    chapters.append({
                        'title': chapter_title,
                        'content': content,
                        'url': url,
                        'word_count': len(content.split()),
                        'char_count': len(content)
                    })
                    
                    # æ›´æ–°çµ±è¨ˆ
                    self.stats['successful_urls'].append(url)
                    self.stats['total_characters'] += len(content)
                    self.stats['total_words'] += len(content.split())
                    
                    print(f"âœ… æˆåŠŸçˆ¬å–ï¼š{chapter_title}")
                    print(f"   ğŸ“Š å­—ç¬¦æ•¸ï¼š{len(content):,} | è©æ•¸ï¼š{len(content.split()):,}")
                    
                    return True
                else:
                    print(f"âš ï¸ å…§å®¹ç‚ºç©ºï¼Œè·³éï¼š{url}")
                    self.stats['failed_urls'].append(url)
                    self.stats['failed_chapters'] += 1
                    return False
                    
            except (requests.exceptions.ConnectionError, 
                    RemoteDisconnected,  # ä¿®æ­£ï¼šç§»é™¤ requests.exceptions.
                    ConnectionError) as e:
                print(f"âŒ é€£æ¥éŒ¯èª¤ (çˆ¬å–ç« ç¯€ {attempt + 1}/{self.max_retries + 1}): {e}")
                
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    # å•Ÿå‹•è‡ªå‹•æ¢å¾©
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        recovery_result = self.trigger_auto_recovery(url, "scrape_chapter")
                        if recovery_result:
                            return recovery_result
                    
                    print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éæ­¤ç« ç¯€")
                    self.stats['failed_urls'].append(url)
                    self.stats['failed_chapters'] += 1
                    return False
                    
            except requests.exceptions.Timeout as e:
                print(f"âŒ è«‹æ±‚è¶…æ™‚ (çˆ¬å–ç« ç¯€ {attempt + 1}/{self.max_retries + 1}): {e}")
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        recovery_result = self.trigger_auto_recovery(url, "scrape_chapter")
                        if recovery_result:
                            return recovery_result
                    
                    print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éæ­¤ç« ç¯€")
                    self.stats['failed_urls'].append(url)
                    self.stats['failed_chapters'] += 1
                    return False
                    
            except Exception as e:
                print(f"âŒ å…¶ä»–éŒ¯èª¤ (çˆ¬å–ç« ç¯€ {attempt + 1}/{self.max_retries + 1}): {e}")
                if attempt < self.max_retries:
                    print(f"â±ï¸  ç­‰å¾… {self.retry_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(self.retry_delay)
                else:
                    if self.auto_recovery and self.recovery_count < self.max_recoveries:
                        recovery_result = self.trigger_auto_recovery(url, "scrape_chapter")
                        if recovery_result:
                            return recovery_result
                    
                    print("âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éæ­¤ç« ç¯€")
                    self.stats['failed_urls'].append(url)
                    self.stats['failed_chapters'] += 1
                    return False
        
        return False

    def check_existing_book_file(self, start_url):
        """æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰çš„æ›¸ç±æ–‡ä»¶"""
        try:
            # æ ¹æ“šURLç”Ÿæˆå¯èƒ½çš„æ–‡ä»¶åæ¨¡å¼
            parsed_url = urlparse(start_url)
            url_identifier = parsed_url.netloc.replace('.', '_')
            
            # æœç´¢ç•¶å‰ç›®éŒ„ä¸­çš„JSONæ–‡ä»¶
            import glob
            json_files = glob.glob("*.json")
            
            for file_path in json_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    if isinstance(data, list) and len(data) > 0:
                        book_data = data[0]
                        
                        # æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æœ¬æ›¸ï¼ˆé€šéURLåŸŸååˆ¤æ–·ï¼‰
                        if ('instruction' in book_data and 
                            url_identifier in file_path.lower()):
                            return file_path
                            
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æª¢æŸ¥ç¾æœ‰æ–‡ä»¶æ™‚å‡ºéŒ¯ï¼š{e}")
            
        return None

    def load_existing_chapters(self, file_path):
        """è¼‰å…¥ç¾æœ‰æ›¸ç±çš„ç« ç¯€ä¿¡æ¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if isinstance(data, list) and len(data) > 0:
                self.existing_book_data = data[0]
                
                # å¾é é¢é‡å»ºç« ç¯€åˆ—è¡¨
                pages = self.existing_book_data.get('pages', [])
                chapters = []
                
                for i, page in enumerate(pages):
                    lines = page.split('\n\n', 1)
                    if len(lines) >= 2:
                        title = lines[0]
                        content = lines[1]
                    else:
                        title = f"ç¬¬{i+1}ç« "
                        content = page
                    
                    chapters.append({
                        'title': title,
                        'content': content,
                        'url': f"existing_chapter_{i+1}",
                        'word_count': len(content.split()),
                        'char_count': len(content)
                    })
                    
                    # è¨˜éŒ„å·²å­˜åœ¨çš„URLï¼ˆæ¨¡æ“¬ï¼‰
                    self.existing_urls.add(f"existing_chapter_{i+1}")
                
                self.existing_chapters = chapters.copy()
                return chapters
                
        except Exception as e:
            print(f"âŒ è¼‰å…¥ç¾æœ‰æ–‡ä»¶å¤±æ•—ï¼š{e}")
            
        return []

    def find_continue_url(self, start_url, existing_chapters):
        """æ‰¾åˆ°æ‡‰è©²ç¹¼çºŒçˆ¬å–çš„URL"""
        if not existing_chapters:
            return start_url
            
        print(f"ğŸ” å°‹æ‰¾çºŒå‚³URLï¼Œå·²æœ‰ {len(existing_chapters)} ç« ")
        
        # å¾èµ·å§‹URLé–‹å§‹ï¼Œè·³éå·²å­˜åœ¨çš„ç« ç¯€æ•¸
        current_url = start_url
        skip_count = len(existing_chapters)
        
        print(f"ğŸ“– éœ€è¦è·³é {skip_count} ç« ")
        
        for i in range(skip_count):
            try:
                response = self.session.get(current_url, timeout=10)
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ç²å–ç•¶å‰ç« ç¯€æ¨™é¡Œé€²è¡Œé©—è­‰
                chapter_title, _ = self.extract_chapter_content(soup, i + 1)
                print(f"   è·³éç¬¬ {i+1} ç« ï¼š{chapter_title}")
                
                # æ‰¾åˆ°ä¸‹ä¸€ç« 
                next_url = self.find_next_page_url(soup, current_url)
                if next_url:
                    current_url = next_url
                    time.sleep(self.delay)  # é¿å…è«‹æ±‚éå¿«
                else:
                    print("ğŸ“„ æ²’æœ‰æ‰¾åˆ°æ›´å¤šç« ç¯€ï¼Œçˆ¬å–å·²å®Œæˆ")
                    return None
                    
            except Exception as e:
                print(f"âŒ è·³éç« ç¯€æ™‚å‡ºéŒ¯ï¼š{e}")
                return start_url
        
        print(f"âœ… æ‰¾åˆ°çºŒå‚³èµ·é»ï¼šç¬¬ {skip_count + 1} ç« ")
        return current_url

    def auto_save_book(self, ebook_data, is_complete=True, is_continue=False):
        """è‡ªå‹•ä¿å­˜æ›¸ç±ï¼ˆæ”¯æ´çºŒå‚³æ¨¡å¼ï¼‰"""
        safe_title = re.sub(r'[^\w\s-]', '', ebook_data['title'])
        safe_title = safe_title.replace(' ', '_')[:50]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        if (is_continue):
            # çºŒå‚³æ¨¡å¼ï¼šæ›´æ–°åŸæ–‡ä»¶åï¼Œä½†åŠ ä¸Šæ–°çš„æ™‚é–“æˆ³
            status_suffix = "updated_complete" if is_complete else "updated_partial"
            filename = f"{safe_title}_{status_suffix}_{timestamp}.json"
            print(f"ğŸ”„ çºŒå‚³æ¨¡å¼ï¼šæ›´æ–°æ›¸ç±æ–‡ä»¶")
        else:
            # æ–°æ›¸æ¨¡å¼
            status_suffix = "complete" if is_complete else "partial"
            filename = f"{safe_title}_{status_suffix}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump([ebook_data], f, ensure_ascii=False, indent=2)
            
            file_size = os.path.getsize(filename) / 1024
            print(f"\nğŸ’¾ æ›¸ç±å·²ä¿å­˜åˆ°ï¼š{filename}")
            print(f"ğŸ“ å®Œæ•´è·¯å¾‘ï¼š{os.path.abspath(filename)}")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°ï¼š{file_size:.1f} KB")
            
            if is_continue:
                print(f"ğŸ”„ çºŒå‚³å®Œæˆï¼šæ–°å¢äº† {len(ebook_data['pages']) - len(self.existing_book_data.get('pages', []))} é ")
            
            if not is_complete:
                print("âš ï¸  æ³¨æ„ï¼šé€™æ˜¯éƒ¨åˆ†å®Œæˆçš„æ›¸ç±ï¼Œå¯èƒ½é‚„æœ‰æ›´å¤šç« ç¯€")
                print("ğŸ’¡ å»ºè­°ï¼šç¨å¾Œé‡æ–°é‹è¡Œè…³æœ¬é€²è¡ŒçºŒå‚³")
            
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—ï¼š{e}")
            return None

    def extract_chapter_content(self, soup, chapter_num):
        """æ™ºèƒ½æå–ç« ç¯€æ¨™é¡Œå’Œå…§å®¹"""
        # å¸¸è¦‹çš„æ¨™é¡Œé¸æ“‡å™¨
        title_selectors = [
            'h1', 'h2', 'h3',
            '.title', '.chapter-title', '.readtitle h1',
            '.j_chapterName', '.chapter_name',
            '.bookname h1', '.book-title'
        ]
        
        # å¸¸è¦‹çš„å…§å®¹é¸æ“‡å™¨
        content_selectors = [
            '.content', '#content', '.chapter-content',
            '.novel-content', '.read-content', '#chapter_content',
            '.text', '.txt', '.detail', '.main-text',
            'div[id*="content"]', 'div[class*="content"]'
        ]
        
        # æå–æ¨™é¡Œ
        chapter_title = f"ç¬¬{chapter_num}ç« "
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                title_text = title_element.get_text().strip()
                if title_text and len(title_text) < 200:  # æ¨™é¡Œä¸æ‡‰è©²å¤ªé•·
                    chapter_title = title_text
                    break
        
        # æå–å…§å®¹ - ä¿®æ”¹é€™éƒ¨åˆ†ä¾†æ­£ç¢ºè™•ç† <p> æ¨™ç±¤
        content = ""
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                # ç§»é™¤è…³æœ¬å’Œæ¨£å¼æ¨™ç±¤
                for script in content_element(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                # ğŸ”§ æ–°å¢ï¼šå°ˆé–€è™•ç† <p> æ¨™ç±¤ä»¥ä¿ç•™åˆ†è¡Œ
                content = self.extract_content_with_paragraphs(content_element)
                content = self.clean_content(content)
                
                # æª¢æŸ¥å…§å®¹é•·åº¦ï¼Œå¤ªçŸ­å¯èƒ½ä¸æ˜¯æ­£æ–‡
                if len(content) > 200:
                    break
        
        return chapter_title, content
    
    def extract_content_with_paragraphs(self, content_element):
        """å°ˆé–€è™•ç† <p> æ¨™ç±¤ï¼Œä¿ç•™æ®µè½åˆ†è¡Œ"""
        # æ‰¾åˆ°æ‰€æœ‰ <p> æ¨™ç±¤
        paragraphs = content_element.find_all('p')
        
        if paragraphs:
            # å¦‚æœæœ‰ <p> æ¨™ç±¤ï¼Œé€å€‹è™•ç†
            paragraph_texts = []
            for p in paragraphs:
                text = p.get_text().strip()
                if text:  # åªæ·»åŠ éç©ºæ®µè½
                    paragraph_texts.append(text)
            
            # ç”¨é›™æ›è¡Œåˆ†éš”æ®µè½
            return '\n\n'.join(paragraph_texts)
        else:
            # å¦‚æœæ²’æœ‰ <p> æ¨™ç±¤ï¼Œä½¿ç”¨åŸæœ‰é‚è¼¯
            return content_element.get_text()

    def clean_content(self, text):
        """æ¸…ç†æ–‡æœ¬å…§å®¹ï¼ˆä¿ç•™åˆ†è¡Œæ ¼å¼ï¼‰"""
        # é¦–å…ˆç§»é™¤å¸¸è¦‹çš„å»£å‘Šæ–‡å­—å’Œç„¡é—œå…§å®¹
        ad_patterns = [
            r'.*?ç« ç¯€éŒ¯èª¤.*?',
            r'.*?èˆ‰å ±.*?',
            r'.*?æ”¶è—.*?',
            r'.*?æŠ•ç¥¨.*?',
            r'.*?æ¨è–¦.*?',
            r'.*?å»£å‘Š.*?',
            r'.*?å…è²»é–±è®€.*?',
            r'.*?é»æ“Šé€²å…¥.*?',
            r'.*?æ›´å¤šç²¾å½©.*?',
            r'æœ¬ç« æœªå®Œ.*?é»æ“Šä¸‹ä¸€é ç¹¼çºŒé–±è®€.*?',
        ]
        
        for pattern in ad_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ğŸ”§ ä¿®æ”¹ï¼šæ›´å¥½åœ°è™•ç†æ®µè½é–“è·
        # ä¿ç•™ç”± extract_content_with_paragraphs ç”¢ç”Ÿçš„é›™æ›è¡Œ
        # ä½†æ¸…ç†å¤šé¤˜çš„ç©ºç™½è¡Œï¼ˆè¶…éå…©å€‹æ›è¡Œçš„æƒ…æ³ï¼‰
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # æ¸…ç†æ¯è¡Œé–‹é ­å’Œçµå°¾çš„ç©ºç™½ï¼Œä½†ä¿ç•™æ›è¡Œ
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # æ¸…ç†æ¯è¡Œçš„é¦–å°¾ç©ºç™½å’Œå¤šé¤˜çš„ç©ºæ ¼
            cleaned_line = re.sub(r'[ \t]+', ' ', line.strip())
            cleaned_lines.append(cleaned_line)
        
        # é‡æ–°çµ„åˆï¼Œä¿ç•™åˆ†è¡Œ
        text = '\n'.join(cleaned_lines)
        
        # ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½è¡Œ
        text = text.strip()
        
        # ğŸ”§ ä¿®æ”¹ï¼šç¢ºä¿æ®µè½ä¹‹é–“ä¿æŒé›™æ›è¡Œ
        # å°‡å–®å€‹æ›è¡Œå¾Œè·Ÿéç©ºè¡Œçš„æƒ…æ³è½‰æ›ç‚ºé›™æ›è¡Œï¼ˆå¦‚æœä¸æ˜¯å·²ç¶“æ˜¯é›™æ›è¡Œï¼‰
        text = re.sub(r'(?<!\n)\n(?!\n)(?=\S)', '\n\n', text)
        
        # æœ€å¾Œæ¸…ç†ï¼šå°‡å¤šæ–¼å…©å€‹çš„é€£çºŒæ›è¡Œç¸®æ¸›ç‚ºå…©å€‹
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text

    def find_next_page_url(self, soup, current_url):
        """æ™ºèƒ½å°‹æ‰¾ä¸‹ä¸€é é€£çµ"""
        # å¸¸è¦‹çš„ä¸‹ä¸€é é¸æ“‡å™¨å’Œæ–‡å­—
        next_selectors = [
            'a[title*="ä¸‹ä¸€"]', 'a[title*="ä¸‹ä¸€é "]', 'a[title*="ä¸‹ä¸€ç« "]',
            'a:contains("ä¸‹ä¸€")', 'a:contains("ä¸‹ä¸€é ")', 'a:contains("ä¸‹ä¸€ç« ")',
            '.next', 'a.next', '#next', 'a#next',
            'a[id*="next"]', 'a[class*="next"]',
            '.chapter-nav .next', '.page-nav .next',
            'a#j_chapterNext', '.j_chapterNext'
        ]
        
        # ä¹Ÿå˜—è©¦æ–‡å­—åŒ¹é…
        next_keywords = ["ä¸‹ä¸€é ", "ä¸‹ä¸€ç« ", "ä¸‹é ", "ä¸‹ç« ", "next", "Next"]
        
        # é¦–å…ˆå˜—è©¦å¸¸è¦‹é¸æ“‡å™¨
        for selector in next_selectors:
            try:
                next_element = soup.select_one(selector)
                if next_element and next_element.get('href'):
                    href = next_element.get('href')
                    if href and href != '#' and href != 'javascript:void(0)':
                        return urljoin(current_url, href)
            except:
                continue
        
        # å¦‚æœé¸æ“‡å™¨æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æ–‡å­—åŒ¹é…
        for keyword in next_keywords:
            links = soup.find_all('a', string=re.compile(keyword, re.I))
            for link in links:
                href = link.get('href')
                if href and href != '#' and href != 'javascript:void(0)':
                    return urljoin(current_url, href)
        
        # æœ€å¾Œå˜—è©¦åŒ…å«é—œéµå­—çš„é€£çµ
        for keyword in next_keywords:
            links = soup.find_all('a', attrs={'title': re.compile(keyword, re.I)})
            for link in links:
                href = link.get('href')
                if href and href != '#' and href != 'javascript:void(0)':
                    return urljoin(current_url, href)
        
        return None
    
    def extract_book_info(self, start_url, first_chapter):
        """å¾URLå’Œç¬¬ä¸€ç« æå–æ›¸åå’Œä½œè€…"""
        # å¾URLå˜—è©¦æå–æ›¸å
        parsed_url = urlparse(start_url)
        url_parts = parsed_url.path.split('/')
        
        # é è¨­å€¼
        book_title = "æœªçŸ¥æ›¸å"
        author = "æœªçŸ¥ä½œè€…"
        
        # å˜—è©¦å¾ç¬¬ä¸€ç« æ¨™é¡Œæ¨æ–·æ›¸å
        if first_chapter['title']:
            title = first_chapter['title']
            # ç§»é™¤å¸¸è¦‹çš„ç« ç¯€æ¨™è¨˜
            title = re.sub(r'ç¬¬?\d+[ç« èŠ‚]', '', title)
            title = re.sub(r'chapter\s*\d+', '', title, flags=re.I)
            title = title.strip()
            if title:
                book_title = title
        
        # å¾URLè·¯å¾‘å˜—è©¦æå–
        for part in url_parts:
            if part and len(part) > 2 and part not in ['book', 'novel', 'chapter', 'read']:
                book_title = part.replace('-', ' ').replace('_', ' ')
                break
        
        return book_title, author
    
    def clean_content(self, text):
        """æ¸…ç†æ–‡æœ¬å…§å®¹ï¼ˆä¿ç•™åˆ†è¡Œæ ¼å¼ï¼‰"""
        # é¦–å…ˆç§»é™¤å¸¸è¦‹çš„å»£å‘Šæ–‡å­—å’Œç„¡é—œå…§å®¹
        ad_patterns = [
            r'.*?ç« ç¯€éŒ¯èª¤.*?',
            r'.*?èˆ‰å ±.*?',
            r'.*?æ”¶è—.*?',
            r'.*?æŠ•ç¥¨.*?',
            r'.*?æ¨è–¦.*?',
            r'.*?å»£å‘Š.*?',
            r'.*?å…è²»é–±è®€.*?',
            r'.*?é»æ“Šé€²å…¥.*?',
            r'.*?æ›´å¤šç²¾å½©.*?',
            r'æœ¬ç« æœªå®Œ.*?é»æ“Šä¸‹ä¸€é ç¹¼çºŒé–±è®€.*?',
        ]
        
        for pattern in ad_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ğŸ”§ ä¿®æ”¹ï¼šæ›´å¥½åœ°è™•ç†æ®µè½é–“è·
        # ä¿ç•™ç”± extract_content_with_paragraphs ç”¢ç”Ÿçš„é›™æ›è¡Œ
        # ä½†æ¸…ç†å¤šé¤˜çš„ç©ºç™½è¡Œï¼ˆè¶…éå…©å€‹æ›è¡Œçš„æƒ…æ³ï¼‰
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # æ¸…ç†æ¯è¡Œé–‹é ­å’Œçµå°¾çš„ç©ºç™½ï¼Œä½†ä¿ç•™æ›è¡Œ
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # æ¸…ç†æ¯è¡Œçš„é¦–å°¾ç©ºç™½å’Œå¤šé¤˜çš„ç©ºæ ¼
            cleaned_line = re.sub(r'[ \t]+', ' ', line.strip())
            cleaned_lines.append(cleaned_line)
        
        # é‡æ–°çµ„åˆï¼Œä¿ç•™åˆ†è¡Œ
        text = '\n'.join(cleaned_lines)
        
        # ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½è¡Œ
        text = text.strip()
        
        # ğŸ”§ ä¿®æ”¹ï¼šç¢ºä¿æ®µè½ä¹‹é–“ä¿æŒé›™æ›è¡Œ
        # å°‡å–®å€‹æ›è¡Œå¾Œè·Ÿéç©ºè¡Œçš„æƒ…æ³è½‰æ›ç‚ºé›™æ›è¡Œï¼ˆå¦‚æœä¸æ˜¯å·²ç¶“æ˜¯é›™æ›è¡Œï¼‰
        text = re.sub(r'(?<!\n)\n(?!\n)(?=\S)', '\n\n', text)
        
        # æœ€å¾Œæ¸…ç†ï¼šå°‡å¤šæ–¼å…©å€‹çš„é€£çºŒæ›è¡Œç¸®æ¸›ç‚ºå…©å€‹
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text
    
    def convert_to_ebook(self, title, author, chapters):
        """è½‰æ›ç‚º Ebook æ ¼å¼"""
        # å°‡ç« ç¯€å…§å®¹åˆ†é 
        pages = []
        
        for chapter in chapters:
            content = chapter['content']
            chapter_title = chapter['title']
            
            # æ¯é æœ€å¤š2000å­—
            max_chars_per_page = 2000
            
            if len(content) <= max_chars_per_page:
                # çŸ­ç« ç¯€ï¼Œæ•´ç« ä½œç‚ºä¸€é 
                pages.append(f"{chapter_title}\n\n{content}")
            else:
                # é•·ç« ç¯€ï¼Œæ™ºèƒ½åˆ†é 
                paragraphs = content.split('\n\n')
                current_page = f"{chapter_title}\n\n"
                current_length = len(current_page)
                
                for paragraph in paragraphs:
                    if current_length + len(paragraph) + 2 > max_chars_per_page and current_page.strip() != chapter_title:
                        pages.append(current_page.strip())
                        current_page = paragraph + "\n\n"
                        current_length = len(current_page)
                    else:
                        current_page += paragraph + "\n\n"
                        current_length += len(paragraph) + 2
                
                if current_page.strip():
                    pages.append(current_page.strip())
        
        # ç”Ÿæˆæ›¸ç±ID
        book_id = f"scraped_{title.replace(' ', '_').lower()}"
        
        ebook_data = {
            "id": book_id,
            "title": title,
            "author": author,
            "coverImage": "default_cover",
            "instruction": f"å¾ç¶²è·¯çˆ¬å–çš„æ›¸ç±ï¼š{title}ï¼Œä½œè€…ï¼š{author}ã€‚å…±{len(chapters)}ç« ï¼Œ{len(pages)}é ã€‚",
            "pages": pages,
            "totalPages": len(pages),
            "currentPage": 0,
            "bookmarkedPages": []
        }
        
        return ebook_data

    def print_scraping_summary(self, chapters):
        """é¡¯ç¤ºè©³ç´°çš„çˆ¬å–ç¸½çµï¼ˆåŒ…å«æ¢å¾©çµ±è¨ˆï¼‰"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        print("\n" + "=" * 80)
        if self.continue_mode:
            print("ğŸ”„ çºŒå‚³å®Œæˆï¼è©³ç´°çµ±è¨ˆå ±å‘Š")
        else:
            print("ğŸ‰ çˆ¬å–å®Œæˆï¼è©³ç´°çµ±è¨ˆå ±å‘Š")
        print("=" * 80)
        
        # åŸºæœ¬çµ±è¨ˆ
        print("ğŸ“Š åŸºæœ¬çµ±è¨ˆï¼š")
        print(f"   â° æœ¬æ¬¡è€—æ™‚ï¼š{duration:.2f} ç§’ ({duration/60:.1f} åˆ†é˜)")
        
        if self.continue_mode:
            new_chapters = len(chapters) - len(self.existing_chapters)
            print(f"   ğŸ“š ç¸½ç« ç¯€ï¼š{len(chapters)} ç« ")
            print(f"   ğŸ“– å·²æœ‰ç« ç¯€ï¼š{len(self.existing_chapters)} ç« ")
            print(f"   ğŸ†• æ–°å¢ç« ç¯€ï¼š{new_chapters} ç« ")
            print(f"   âŒ å¤±æ•—ç« ç¯€ï¼š{self.stats['failed_chapters']} ç« ")
        else:
            print(f"   ğŸ“š æˆåŠŸç« ç¯€ï¼š{self.stats['total_chapters']} ç« ")
            print(f"   âŒ å¤±æ•—ç« ç¯€ï¼š{self.stats['failed_chapters']} ç« ")
        
        # æ–°å¢æ¢å¾©çµ±è¨ˆ
        if self.recovery_count > 0:
            print(f"   ğŸ”„ è‡ªå‹•æ¢å¾©æ¬¡æ•¸ï¼š{self.recovery_count} æ¬¡")
        
        print(f"   ğŸŒ è¨ªå•URLæ•¸ï¼š{len(self.stats['visited_urls'])}")
        print(f"   âœ… æˆåŠŸç‡ï¼š{(self.stats['total_chapters']/(self.stats['total_chapters']+self.stats['failed_chapters'])*100):.1f}%" if (self.stats['total_chapters']+self.stats['failed_chapters']) > 0 else "   âœ… æˆåŠŸç‡ï¼š0%")
        
        # å…§å®¹çµ±è¨ˆ
        print("\nğŸ“ å…§å®¹çµ±è¨ˆï¼š")
        if self.continue_mode:
            print(f"   ğŸ“„ ç¸½å­—ç¬¦æ•¸ï¼š{self.stats['total_characters']:,} (æœ¬æ¬¡æ–°å¢)")
            print(f"   ğŸ“ ç¸½è©æ•¸ï¼š{self.stats['total_words']:,} (æœ¬æ¬¡æ–°å¢)")
        else:
            print(f"   ğŸ“„ ç¸½å­—ç¬¦æ•¸ï¼š{self.stats['total_characters']:,}")
            print(f"   ğŸ“ ç¸½è©æ•¸ï¼š{self.stats['total_words']:,}")
        
        if self.stats['total_chapters'] > 0:
            avg_chars = self.stats['total_characters'] / self.stats['total_chapters']
            avg_words = self.stats['total_words'] / self.stats['total_chapters']
            print(f"   ğŸ“Š å¹³å‡æ¯ç« å­—ç¬¦ï¼š{avg_chars:,.0f}")
            print(f"   ğŸ“Š å¹³å‡æ¯ç« è©æ•¸ï¼š{avg_words:,.0f}")
        
        # ç« ç¯€è©³æƒ…
        if chapters:
            print("\nğŸ“– ç« ç¯€è©³æƒ…ï¼š")
            if self.continue_mode and len(self.existing_chapters) > 0:
                print(f"   (å·²æœ‰ {len(self.existing_chapters)} ç« ï¼Œä»¥ä¸‹ç‚ºæ–°å¢ç« ç¯€)")
                start_index = len(self.existing_chapters)
                display_chapters = chapters[start_index:start_index+10]
                for i, chapter in enumerate(display_chapters, start_index + 1):
                    print(f"   {i:2d}. {chapter['title'][:50]}{'...' if len(chapter['title']) > 50 else ''}")
                    print(f"       ğŸ“Š {chapter['char_count']:,} å­—ç¬¦ | {chapter['word_count']:,} è©")
            else:
                for i, chapter in enumerate(chapters[:10], 1):
                    print(f"   {i:2d}. {chapter['title'][:50]}{'...' if len(chapter['title']) > 50 else ''}")
                    print(f"       ğŸ“Š {chapter['char_count']:,} å­—ç¬¦ | {chapter['word_count']:,} è©")
            
            if len(chapters) > 10:
                print(f"   ... é‚„æœ‰ {len(chapters) - 10} ç« ")
        
        # æ•ˆç‡çµ±è¨ˆ
        print("\nâš¡ æ•ˆç‡çµ±è¨ˆï¼š")
        if duration > 0:
            chars_per_sec = self.stats['total_characters'] / duration
            chapters_per_min = (self.stats['total_chapters'] / duration) * 60
            print(f"   ğŸš€ çˆ¬å–é€Ÿåº¦ï¼š{chars_per_sec:,.0f} å­—ç¬¦/ç§’")
            print(f"   ğŸ“š ç« ç¯€é€Ÿåº¦ï¼š{chapters_per_min:.1f} ç« /åˆ†é˜")
        
        # å¤±æ•—URLï¼ˆå¦‚æœæœ‰ï¼‰
        if self.stats['failed_urls']:
            print("\nâŒ å¤±æ•—çš„URLï¼š")
            for i, url in enumerate(self.stats['failed_urls'][:5], 1):
                print(f"   {i}. {url}")
            if len(self.stats['failed_urls']) > 5:
                print(f"   ... é‚„æœ‰ {len(self.stats['failed_urls']) - 5} å€‹å¤±æ•—URL")
        
        print("=" * 80)

def main():
    """ä¸»å‡½æ•¸ï¼šå¾ç”¨æˆ¶è¼¸å…¥çš„URLé–‹å§‹çˆ¬å–ï¼ˆæ”¯æ´çºŒå‚³å’Œè‡ªå‹•æ¢å¾©ï¼‰"""
    scraper = UniversalBookScraper()
    
    print("ğŸ“š Universal Book Scraper v2.3 (with Auto-Recovery)")
    print("=" * 50)
    
    # ç²å–ç”¨æˆ¶è¼¸å…¥ - æ”¹é€²è¼¸å…¥é©—è­‰
    while True:
        start_url = input("è«‹è¼¸å…¥ç¬¬ä¸€ç« çš„URLï¼š").strip()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©º
        if not start_url:
            print("âŒ URLä¸èƒ½ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥")
            continue
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡æˆ–ç„¡æ•ˆå­—ç¬¦
        if any('\u4e00' <= char <= '\u9fff' for char in start_url):
            print("âŒ URLåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œè«‹è¼¸å…¥æ­£ç¢ºçš„è‹±æ–‡URL")
            print("ğŸ’¡ ä¾‹å¦‚ï¼šhttps://example.com/book/chapter1.html")
            continue
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æç¤ºæ–‡å­—
        if "è«‹è¼¸å…¥" in start_url or "URL" in start_url:
            print("âŒ è«‹ä¸è¦åŒ…å«æç¤ºæ–‡å­—ï¼Œåªè¼¸å…¥å¯¦éš›çš„ç¶²å€")
            print("ğŸ’¡ ä¾‹å¦‚ï¼šhttps://example.com/book/chapter1.html")
            continue
        
        # ç¢ºä¿URLæœ‰å”è­°
        if not start_url.startswith(('http://', 'https://')):
            start_url = 'https://' + start_url
            print(f"ğŸ”§ å·²è‡ªå‹•æ·»åŠ  https:// å‰ç¶´ï¼š{start_url}")
        
        # é©—è­‰URLæ ¼å¼
        try:
            parsed = urlparse(start_url)
            if not parsed.netloc:
                print("âŒ URLæ ¼å¼ä¸æ­£ç¢ºï¼Œè«‹è¼¸å…¥å®Œæ•´çš„ç¶²å€")
                print("ğŸ’¡ ä¾‹å¦‚ï¼šhttps://example.com/book/chapter1.html")
                continue
            break
        except Exception as e:
            print(f"âŒ URLæ ¼å¼éŒ¯èª¤ï¼š{e}")
            print("ğŸ’¡ è«‹è¼¸å…¥æ­£ç¢ºçš„ç¶²å€æ ¼å¼ï¼Œä¾‹å¦‚ï¼šhttps://example.com/book/chapter1.html")
            continue
    
    max_chapters = 999
    
    print(f"\nğŸš€ é–‹å§‹çˆ¬å–ï¼š{start_url}")
    print(f"ğŸ“– è‡ªå‹•çˆ¬å–æ‰€æœ‰å¯ç”¨ç« ç¯€")
    print(f"ğŸ”„ é‡è©¦æ©Ÿåˆ¶ï¼šæœ€å¤šé‡è©¦ {scraper.max_retries} æ¬¡")
    print(f"â±ï¸  é‡è©¦å»¶é²ï¼š{scraper.retry_delay} ç§’")
    print(f"ğŸ›¡ï¸  è‡ªå‹•æ¢å¾©ï¼šå•Ÿç”¨ï¼ˆæœ€å¤š {scraper.max_recoveries} æ¬¡ï¼Œç­‰å¾… {scraper.recovery_delay} ç§’ï¼‰")
    print(f"ğŸ“‚ çºŒå‚³åŠŸèƒ½ï¼šè‡ªå‹•æª¢æ¸¬å·²å­˜åœ¨çš„æ–‡ä»¶")
    print("-" * 50)
    
    # é–‹å§‹çˆ¬å–
    try:
        ebook_data = scraper.scrape_from_url(start_url, max_chapters)
        
        if ebook_data:
            # æ–‡ä»¶åè™•ç†
            safe_title = re.sub(r'[^\w\s-]', '', ebook_data['title'])
            safe_title = safe_title.replace(' ', '_')[:50]  # é™åˆ¶æ–‡ä»¶åé•·åº¦
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"{safe_title}_{timestamp}.json"
            
            # ä¿å­˜æ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump([ebook_data], f, ensure_ascii=False, indent=2)
            
            # æœ€çµ‚ç¸½çµ
            print("\nğŸŠ ä»»å‹™å®Œæˆï¼æœ€çµ‚å ±å‘Š")
            print("=" * 60)
            print(f"ğŸ“š æ›¸åï¼š{ebook_data['title']}")
            print(f"ğŸ‘¤ ä½œè€…ï¼š{ebook_data['author']}")
            print(f"ğŸ“„ ç¸½é æ•¸ï¼š{len(ebook_data['pages'])} é ")
            print(f"ğŸ“– ç¸½ç« ç¯€ï¼š{scraper.stats['total_chapters']} ç« ")
            
            if scraper.continue_mode:
                existing_pages = len(scraper.existing_book_data.get('pages', []))
                new_pages = len(ebook_data['pages']) - existing_pages
                print(f"ğŸ”„ çºŒå‚³çµæœï¼šæ–°å¢ {new_pages} é ")
            
            print(f"ğŸ“ ç¸½å­—ç¬¦ï¼š{scraper.stats['total_characters']:,}")
            print(f"ğŸ“Š ç¸½è©æ•¸ï¼š{scraper.stats['total_words']:,}")
            print(f"ğŸ’¾ æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(filename) / 1024:.1f} KB")
            print(f"ğŸ’¾ ä¿å­˜ç‚ºï¼š{filename}")
            print(f"ğŸ“ ä¿å­˜è·¯å¾‘ï¼š{os.path.abspath(filename)}")
            
            if scraper.stats['failed_chapters'] > 0:
                print(f"âš ï¸  å¤±æ•—ç« ç¯€ï¼š{scraper.stats['failed_chapters']} ç« ")
                print("ğŸ’¡ å»ºè­°ç¨å¾Œé‡æ–°é‹è¡Œè…³æœ¬é€²è¡ŒçºŒå‚³")
            
            print("\nğŸ’¡ çºŒå‚³åŠŸèƒ½èªªæ˜ï¼š")
            print("   1. ä¸‹æ¬¡é‹è¡Œæ™‚è¼¸å…¥ç›¸åŒçš„èµ·å§‹URL")
            print("   2. è…³æœ¬æœƒè‡ªå‹•æª¢æ¸¬å·²å­˜åœ¨çš„æ–‡ä»¶")
            print("   3. è·³éå·²çˆ¬å–çš„ç« ç¯€ï¼Œç¹¼çºŒæœªå®Œæˆçš„éƒ¨åˆ†")
            print("   4. é©åˆè™•ç†å¤§å‹æ›¸ç±æˆ–ç¶²çµ¡ä¸­æ–·çš„æƒ…æ³")
            
            print("=" * 60)
            return filename
        else:
            print("\nâŒ çˆ¬å–å¤±æ•—ï¼Œæ²’æœ‰ç²å–åˆ°ä»»ä½•å…§å®¹")
            return None
            
    except Exception as e:
        print(f"\nâŒ çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆï¼š")
        print("   1. æª¢æŸ¥ç¶²çµ¡é€£æ¥")
        print("   2. ç¢ºèªURLæ˜¯å¦æ­£ç¢º")
        print("   3. å˜—è©¦é‡æ–°é‹è¡Œè…³æœ¬")
        print("   4. æª¢æŸ¥ç¶²ç«™æ˜¯å¦æœ‰åçˆ¬èŸ²ä¿è­·")
        return False

if __name__ == "__main__":
    main()
